{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8d651bc-1774-41e2-99ff-fc019bfb9610",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data Science Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Processing\n",
    "import itertools\n",
    "import functools\n",
    "from functools import lru_cache\n",
    "\n",
    "# Output to Excel\n",
    "from shutil import copyfile\n",
    "\n",
    "# Visualization\n",
    "from IPython.display import display, HTML, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f46d444a-b71d-4d27-9865-fa3e6797cbcf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Main User Interface: https://ftgdev.service-now.com/x/fof/loss-triangles-2/loss-triangles-page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a68832bf-393d-4897-a958-8b6e5f37237f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def load_data(\n",
    "    CLAIMS_LOCATION: str = \"\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load insurance data from the provided file paths.\n",
    "\n",
    "    This function attempts to load dataframes from the provided paths in the order:\n",
    "    Parquet, CSV, and then Excel. If successful in one format, it stops further attempts.\n",
    "\n",
    "    Parameters:\n",
    "    - CLAIMS_LOCATION (str): Path to the claims data file.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The Claims dataframe.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If the files are not in one of the expected formats.\n",
    "\n",
    "    Examples:\n",
    "    >>> premiums, claims = load_data(\"premiums.parquet\", \"claims.parquet\")\n",
    "    Dataset was in PARQUET format.\n",
    "    \"\"\"\n",
    "    formats = [\n",
    "        (\"parquet\", pd.read_parquet),\n",
    "        (\"csv\", pd.read_csv),\n",
    "        (\"excel\", pd.read_excel),\n",
    "    ]\n",
    "\n",
    "    for file_format, loader in formats:\n",
    "        try:\n",
    "            claims_df = loader(CLAIMS_LOCATION)\n",
    "            print(f\"Dataset was in {file_format.upper()} format.\")\n",
    "            return claims_df\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Failed to load data in {file_format.upper()} format due to: {str(e)}\"\n",
    "            )\n",
    "\n",
    "    raise ValueError(\n",
    "        \"Claims file is not in an accepted format of: Parquet, CSV, or Excel.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8082f559-071c-4d33-b721-d9f1106d5fdb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_static_data(\n",
    "    BOOK: str,\n",
    ") -> tuple[pd.DataFrame, str, str, list[str],]:\n",
    "    if BOOK == \"King\":\n",
    "        CLAIMS_LOCATION = \"/dbfs/FileStore/ID/kmh_book_10_04_23.xlsx\"\n",
    "        FULL_CLAIMS_DF = load_data(CLAIMS_LOCATION).rename(\n",
    "            columns={\n",
    "                \"OS_loss\": \"OS_Loss\",\n",
    "            }\n",
    "        )\n",
    "        X_AXIS_DATE_FIELD = \"Accounting_Date\"\n",
    "        CLAIM_NUMBER_COLUMN = \"claim_nbr\"\n",
    "        LOSS_COLUMNS = [\"Paid_Loss\", \"Paid_ALE\", \"OS_Loss\", \"OS_ALE\"]\n",
    "\n",
    "    elif BOOK == \"Park\":\n",
    "        CLAIMS_LOCATION = \"/dbfs/FileStore/ID/kp_book_10_05_23.xlsx\"\n",
    "        FULL_CLAIMS_DF = load_data(CLAIMS_LOCATION).rename(\n",
    "            columns={\n",
    "                \"OS_loss\": \"OS_Loss\",\n",
    "            }\n",
    "        )\n",
    "        FULL_CLAIMS_DF = FULL_CLAIMS_DF.loc[FULL_CLAIMS_DF[\"ASL\"] == 5.1]\n",
    "\n",
    "        X_AXIS_DATE_FIELD = \"Accounting_Date\"\n",
    "        CLAIM_NUMBER_COLUMN = \"claim_nbr\"\n",
    "        LOSS_COLUMNS = [\"Paid_Loss\", \"Paid_ALE\", \"OS_Loss\", \"OS_ALE\"]\n",
    "\n",
    "    elif BOOK == \"AVT\":\n",
    "        CLAIMS_LOCATION = \"/dbfs/FileStore/ID/avt_book_10_05_23.xlsx\"\n",
    "        FULL_CLAIMS_DF = load_data(CLAIMS_LOCATION).rename(\n",
    "            columns={\n",
    "                \"acc_date\": \"Acc_Date\",\n",
    "                \"policy_eff_date\": \"Policy_Date\",\n",
    "                \"Reporting_date\": \"Report_Date\",\n",
    "                \"accounting_date\": \"Accounting_Date\",\n",
    "                \"claim_number\": \"claim_nbr\",\n",
    "                \"paid_loss\": \"Paid_Loss\",\n",
    "                \"paid_ale\": \"Paid_ALE\",\n",
    "                \"os_loss\": \"OS_Loss\",\n",
    "                \"os_ale\": \"OS_ALE\",\n",
    "                \"salvage\": \"Salvage\",\n",
    "                \"subrogation\": \"Subrogation\",\n",
    "            }\n",
    "        )\n",
    "        FULL_CLAIMS_DF = FULL_CLAIMS_DF.loc[FULL_CLAIMS_DF[\"NISS_ASL\"] == 51.0]\n",
    "\n",
    "        X_AXIS_DATE_FIELD = \"Accounting_Date\"  # accounting date\n",
    "        CLAIM_NUMBER_COLUMN = \"claim_nbr\"\n",
    "        LOSS_COLUMNS = [\"Paid_Loss\", \"Paid_ALE\", \"OS_Loss\", \"OS_ALE\"]\n",
    "\n",
    "    return (\n",
    "        FULL_CLAIMS_DF,\n",
    "        X_AXIS_DATE_FIELD,\n",
    "        CLAIM_NUMBER_COLUMN,\n",
    "        LOSS_COLUMNS,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d8c737c-2af9-42c1-9f04-bbe326551d9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_time_interval_fields(\n",
    "    df: pd.DataFrame,\n",
    "    X_AXIS_DATE_FIELD: str,\n",
    "    Y_AXIS_DATE_FIELD: str,\n",
    "    TIME_INTERVAL: str = \"quarterly\",\n",
    ") -> tuple[pd.DataFrame, dict]:\n",
    "    df[X_AXIS_DATE_FIELD] = pd.to_datetime(df[X_AXIS_DATE_FIELD])\n",
    "    df[Y_AXIS_DATE_FIELD] = pd.to_datetime(df[Y_AXIS_DATE_FIELD])\n",
    "\n",
    "    time_interval_axis_mapping = {}\n",
    "\n",
    "    if TIME_INTERVAL == \"quarterly\":\n",
    "        x_axis_name = \"Months_Developed_AQ\"\n",
    "        y_axis_name = \"First_AQ\"\n",
    "        df[x_axis_name] = df[X_AXIS_DATE_FIELD].dt.to_period(\"Q\").astype(str)\n",
    "        df[y_axis_name] = df[Y_AXIS_DATE_FIELD].dt.to_period(\"Q\").astype(str)\n",
    "        time_interval_axis_mapping[\"triangle_column_name\"] = \"AQ\"\n",
    "\n",
    "    elif TIME_INTERVAL == \"monthly\":\n",
    "        x_axis_name = \"Months_Developed\"\n",
    "        y_axis_name = \"First_Month\"\n",
    "        df[x_axis_name] = (\n",
    "            df[X_AXIS_DATE_FIELD].dt.year.astype(str)\n",
    "            + \"-\"\n",
    "            + df[X_AXIS_DATE_FIELD].dt.month.astype(str)\n",
    "        )\n",
    "        df[y_axis_name] = (\n",
    "            df[Y_AXIS_DATE_FIELD].dt.year.astype(str)\n",
    "            + \"-\"\n",
    "            + df[Y_AXIS_DATE_FIELD].dt.month.astype(str)\n",
    "        )\n",
    "        time_interval_axis_mapping[\"triangle_column_name\"] = \"AM\"\n",
    "\n",
    "    elif TIME_INTERVAL == \"yearly\":\n",
    "        x_axis_name = \"Years_Developed\"\n",
    "        y_axis_name = \"First_Year\"\n",
    "        df[x_axis_name] = df[X_AXIS_DATE_FIELD].dt.year.astype(str)\n",
    "        df[y_axis_name] = df[Y_AXIS_DATE_FIELD].dt.year.astype(str)\n",
    "        time_interval_axis_mapping[\"triangle_column_name\"] = \"AY\"\n",
    "\n",
    "    else:\n",
    "        assert \"TIME_INTERVAL parameter must be in ['quarterly', 'monthly', 'yearly']\"\n",
    "\n",
    "    time_interval_axis_mapping[\"x-axis\"] = x_axis_name\n",
    "    time_interval_axis_mapping[\"y-axis\"] = y_axis_name\n",
    "\n",
    "    return (df, time_interval_axis_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae4a4478-1770-4e2b-89f8-b79683350431",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_cumulative_metrics(\n",
    "    df: pd.DataFrame,\n",
    "    CLAIM_NUMBER_COLUMN: str = \"claim_nbr\",\n",
    "    LOSS_COLUMNS: list[str] = [\"Paid_Loss\", \"Paid_ALE\", \"OS_Loss\", \"OS_ALE\"],\n",
    "    time_interval_axis_mapping: dict = {},\n",
    "    X_AXIS_DATE_FIELD: str = \"\",\n",
    ") -> pd.DataFrame:\n",
    "    assert (\n",
    "        time_interval_axis_mapping != {}\n",
    "    ), \"time_interval_axis_mapping must not be an empty dict\"\n",
    "\n",
    "    df = df.sort_values(\n",
    "        [\n",
    "            CLAIM_NUMBER_COLUMN,\n",
    "            time_interval_axis_mapping[\"y-axis\"],\n",
    "            time_interval_axis_mapping[\"x-axis\"],\n",
    "            X_AXIS_DATE_FIELD,\n",
    "        ]\n",
    "    )\n",
    "    df[LOSS_COLUMNS] = df.groupby([CLAIM_NUMBER_COLUMN])[LOSS_COLUMNS].cumsum()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "122d3b8c-6095-4785-a03a-b034132cb477",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_last_values_during_developed_period(\n",
    "    df: pd.DataFrame,\n",
    "    CLAIM_NUMBER_COLUMN: str = \"claim_nbr\",\n",
    "    LOSS_COLUMNS: list[str] = [\"Paid_Loss\", \"Paid_ALE\", \"OS_Loss\", \"OS_ALE\"],\n",
    "    time_interval_axis_mapping: dict = {},\n",
    ") -> pd.DataFrame:\n",
    "    assert (\n",
    "        time_interval_axis_mapping != {}\n",
    "    ), \"time_interval_axis_mapping must not be an empty dict\"\n",
    "\n",
    "    agg_dict = {col: \"last\" for col in LOSS_COLUMNS}\n",
    "    df = (\n",
    "        df.groupby(\n",
    "            [\n",
    "                CLAIM_NUMBER_COLUMN,\n",
    "                time_interval_axis_mapping[\"y-axis\"],\n",
    "                time_interval_axis_mapping[\"x-axis\"],\n",
    "            ]\n",
    "        )\n",
    "        .agg(agg_dict)\n",
    "        .reset_index(drop=False)\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0b05578-404f-4311-ab71-ae324c727824",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_period_claim_pivot(\n",
    "    df: pd.DataFrame,\n",
    "    CLAIM_NUMBER_COLUMN: str,\n",
    "    time_interval_axis_mapping: dict = {},\n",
    "    TIME_INTERVAL: str = \"quarterly\",\n",
    ") -> tuple[pd.DataFrame, str, str]:\n",
    "    assert (\n",
    "        time_interval_axis_mapping != {}\n",
    "    ), \"time_interval_axis_mapping must not be an empty dict\"\n",
    "\n",
    "    def _get_period_list(\n",
    "        start_period: str, end_period: str, TIME_INTERVAL: str = \"quarterly\"\n",
    "    ):\n",
    "        freq_time_interval_mapping = {\n",
    "            \"quarterly\": \"Q\",\n",
    "            \"monthly\": \"M\",\n",
    "            \"yearly\": \"Y\",\n",
    "        }\n",
    "        start_period = pd.Period(\n",
    "            start_period, freq=freq_time_interval_mapping[TIME_INTERVAL]\n",
    "        )\n",
    "        end_period = pd.Period(\n",
    "            end_period, freq=freq_time_interval_mapping[TIME_INTERVAL]\n",
    "        )\n",
    "\n",
    "        period_range = pd.period_range(\n",
    "            start=start_period,\n",
    "            end=end_period,\n",
    "            freq=freq_time_interval_mapping[TIME_INTERVAL],\n",
    "        )\n",
    "\n",
    "        period_list = period_range.astype(str).tolist()\n",
    "\n",
    "        return period_list\n",
    "\n",
    "    start_period, end_period = (\n",
    "        min(\n",
    "            df[time_interval_axis_mapping[\"x-axis\"]].min(),\n",
    "            df[time_interval_axis_mapping[\"y-axis\"]].min(),\n",
    "        ),\n",
    "        max(\n",
    "            df[time_interval_axis_mapping[\"x-axis\"]].max(),\n",
    "            df[time_interval_axis_mapping[\"y-axis\"]].max(),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    period_list = _get_period_list(start_period, end_period, TIME_INTERVAL)\n",
    "    claim_nbr_list = df[CLAIM_NUMBER_COLUMN].unique().astype(str).tolist()\n",
    "\n",
    "    period_claim_nbr_list = list(itertools.product(period_list, claim_nbr_list))\n",
    "\n",
    "    df_period_claim_pivot = pd.DataFrame(\n",
    "        period_claim_nbr_list,\n",
    "        columns=[time_interval_axis_mapping[\"x-axis\"], CLAIM_NUMBER_COLUMN],\n",
    "    )\n",
    "\n",
    "    df_quarter_claim_pivot = df_period_claim_pivot.sort_values(\n",
    "        [CLAIM_NUMBER_COLUMN, time_interval_axis_mapping[\"x-axis\"]]\n",
    "    )[[CLAIM_NUMBER_COLUMN, time_interval_axis_mapping[\"x-axis\"]]]\n",
    "\n",
    "    return (df_quarter_claim_pivot, start_period, end_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfd4640e-3aa6-41b7-8de5-8d221ba4d682",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def merge_with_period_claim_pivot(\n",
    "    df: pd.DataFrame,\n",
    "    df_period_claim_pivot: pd.DataFrame,\n",
    "    CLAIM_NUMBER_COLUMN: str,\n",
    "    time_interval_axis_mapping: dict = {},\n",
    ") -> pd.DataFrame:\n",
    "    assert (\n",
    "        time_interval_axis_mapping != {}\n",
    "    ), \"time_interval_axis_mapping must not be an empty dict\"\n",
    "\n",
    "    claims_triangle_df = df.merge(\n",
    "        df_period_claim_pivot,\n",
    "        on=[CLAIM_NUMBER_COLUMN, time_interval_axis_mapping[\"x-axis\"]],\n",
    "        how=\"outer\",\n",
    "    )\n",
    "\n",
    "    claims_triangle_df[time_interval_axis_mapping[\"y-axis\"]] = claims_triangle_df.groupby(\n",
    "        [CLAIM_NUMBER_COLUMN]\n",
    "    )[time_interval_axis_mapping[\"y-axis\"]].ffill()\n",
    "\n",
    "    claims_triangle_df[\n",
    "        [\n",
    "            \"Paid_Loss\",\n",
    "            \"Paid_ALE\",\n",
    "            \"OS_Loss\",\n",
    "            \"OS_ALE\",\n",
    "            \"Total_Paid\",\n",
    "            \"Total_OS\",\n",
    "            \"Total_Incurred\",\n",
    "            \"Total_ALE\",\n",
    "            \"Total_Loss\",\n",
    "        ]\n",
    "    ] = (\n",
    "        claims_triangle_df.sort_values(\n",
    "            [\n",
    "                CLAIM_NUMBER_COLUMN,\n",
    "                time_interval_axis_mapping[\"y-axis\"],\n",
    "                time_interval_axis_mapping[\"x-axis\"],\n",
    "            ]\n",
    "        )\n",
    "        .groupby(CLAIM_NUMBER_COLUMN)[\n",
    "            [\n",
    "                \"Paid_Loss\",\n",
    "                \"Paid_ALE\",\n",
    "                \"OS_Loss\",\n",
    "                \"OS_ALE\",\n",
    "                \"Total_Paid\",\n",
    "                \"Total_OS\",\n",
    "                \"Total_Incurred\",\n",
    "                \"Total_ALE\",\n",
    "                \"Total_Loss\",\n",
    "            ]\n",
    "        ]\n",
    "        .ffill()\n",
    "    )\n",
    "\n",
    "    return claims_triangle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "799359f0-0bde-48d0-b287-bff101defb0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_loss_triangle_on_target_metric(\n",
    "    df: pd.DataFrame,\n",
    "    start_quarter: str,\n",
    "    end_quarter: str,\n",
    "    TARGET_LOSS_METRIC: str = \"Total_Paid\",\n",
    "    TIME_INTERVAL: str = \"quarterly\",\n",
    "    time_interval_axis_mapping: dict = {},\n",
    ") -> tuple[list[list[float]], int, list[str]]:\n",
    "    assert (\n",
    "        time_interval_axis_mapping != {}\n",
    "    ), \"time_interval_axis_mapping must not be an empty dict\"\n",
    "\n",
    "    def _get_period_list(start_period: str, end_period: str, TIME_INTERVAL: str = \"quarterly\"):\n",
    "        freq_time_interval_mapping = {\n",
    "            \"quarterly\": \"Q\",\n",
    "            \"monthly\": \"M\",\n",
    "            \"yearly\": \"Y\",\n",
    "        }\n",
    "        start_period = pd.Period(start_period, freq=freq_time_interval_mapping[TIME_INTERVAL])\n",
    "        end_period = pd.Period(end_period, freq=freq_time_interval_mapping[TIME_INTERVAL])\n",
    "\n",
    "        period_range = pd.period_range(\n",
    "            start=start_period, end=end_period, freq=freq_time_interval_mapping[TIME_INTERVAL]\n",
    "        )\n",
    "\n",
    "        period_list = period_range.astype(str).tolist()\n",
    "\n",
    "        return period_list\n",
    "    \n",
    "    def _get_period_data(y_axis_period: str) -> list[float]:\n",
    "        return [\n",
    "            df.loc[\n",
    "                (df[time_interval_axis_mapping[\"y-axis\"]] == y_axis_period)\n",
    "                & (df[time_interval_axis_mapping[\"x-axis\"]] == x_axis_period)\n",
    "            ][TARGET_LOSS_METRIC].sum()\n",
    "            for x_axis_period in _get_period_list(\n",
    "                y_axis_period, end_quarter, TIME_INTERVAL\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    y_axis_list = _get_period_list(start_quarter, end_quarter, TIME_INTERVAL)\n",
    "    triangle_list = [_get_period_data(y_period) for y_period in y_axis_list]\n",
    "\n",
    "    max_len = max(len(row) for row in triangle_list)\n",
    "    triangle_list = [\n",
    "        row + [np.nan] * (max_len - len(row)) for row in triangle_list\n",
    "    ]\n",
    "\n",
    "    return triangle_list, max_len, y_axis_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d18f907f-615c-4db7-a2de-ca3eea20b63c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_claims_triangle(\n",
    "    triangle_list: list[list[float]],\n",
    "    max_len: int,\n",
    "    y_axis_list: list[str],\n",
    "    TIME_INTERVAL: str = \"quarterly\",\n",
    "    time_interval_axis_mapping: dict = {},\n",
    ") -> pd.DataFrame:\n",
    "    time_interval_to_iteration_mapping = {\"quarterly\": 3, \"monthly\": 1, \"yearly\": 12}\n",
    "    development_month_list = (\n",
    "        np.arange(\n",
    "            time_interval_to_iteration_mapping[TIME_INTERVAL],\n",
    "            (max_len + 1) * time_interval_to_iteration_mapping[TIME_INTERVAL],\n",
    "            time_interval_to_iteration_mapping[TIME_INTERVAL],\n",
    "        )\n",
    "        .astype(int)\n",
    "        .astype(str)\n",
    "        .tolist()\n",
    "    )\n",
    "    dataframe_dtype_dict = {\n",
    "        development_month: float for development_month in development_month_list\n",
    "    }\n",
    "    dataframe_dtype_dict[time_interval_axis_mapping[\"triangle_column_name\"]] = str\n",
    "\n",
    "    claims_triangle = pd.DataFrame(\n",
    "        np.hstack((np.array(y_axis_list).reshape(-1, 1), np.array(triangle_list))),\n",
    "        columns=[time_interval_axis_mapping[\"triangle_column_name\"]]\n",
    "        + development_month_list,\n",
    "    ).astype(dataframe_dtype_dict)\n",
    "    return claims_triangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa7ef4b5-99b0-4be8-84c8-3d8b32594887",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_ldf_from_claims_triangle(\n",
    "    claims_triangle: pd.DataFrame, TIME_INTERVAL: str = \"quarterly\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the loss development factor from the claims triangle.\n",
    "\n",
    "    Parameters:\n",
    "    - triangle_ldf_df (pd.DataFrame): The triangular formatted dataframe.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The dataframe updated with loss development factors.\n",
    "    \"\"\"\n",
    "    time_interval_to_iteration_mapping = {\"quarterly\": 3, \"monthly\": 1, \"yearly\": 12}\n",
    "\n",
    "    triangle_ldf_df = claims_triangle.copy()\n",
    "    columns = triangle_ldf_df.columns[1:]\n",
    "\n",
    "    for i in range(len(columns) - 1):\n",
    "        col_name = columns[i]\n",
    "        next_col_name = columns[i + 1]\n",
    "\n",
    "        triangle_ldf_df[col_name] = 1 / np.where(\n",
    "            triangle_ldf_df[next_col_name] == 0,\n",
    "            np.nan,\n",
    "            triangle_ldf_df[col_name].div(triangle_ldf_df[next_col_name]),\n",
    "        )\n",
    "    triangle_ldf_df[columns[-1]] = np.nan\n",
    "\n",
    "    age_to_age_columns = [\n",
    "        \"{}-{}\".format(\n",
    "            columns[i], int(columns[i]) + time_interval_to_iteration_mapping[TIME_INTERVAL]\n",
    "        )\n",
    "        for i in range(len(columns))\n",
    "    ]\n",
    "    triangle_ldf_df.columns = [triangle_ldf_df.columns[0]] + age_to_age_columns\n",
    "\n",
    "    triangle_ldf_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    return triangle_ldf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e542c83-3fd2-49cb-b2cc-7b86be157271",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_loss_triangles_from_parameters(\n",
    "    df: pd.DataFrame,\n",
    "    X_AXIS_DATE_FIELD: str,\n",
    "    Y_AXIS_DATE_FIELD: str,\n",
    "    CLAIM_NUMBER_COLUMN: str,\n",
    "    LOSS_COLUMNS: list[str],\n",
    "    TARGET_LOSS_METRIC: str,\n",
    "    TIME_INTERVAL: str,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    def _update_insurance_totals(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df[\"Total_Paid\"] = df[\"Paid_Loss\"] + df[\"Paid_ALE\"]\n",
    "        df[\"Total_OS\"] = df[\"OS_Loss\"] + df[\"OS_ALE\"]\n",
    "        df[\"Total_Incurred\"] = df[\"Total_Paid\"] + df[\"Total_OS\"]\n",
    "        df[\"Total_ALE\"] = df[\"Paid_ALE\"] + df[\"OS_ALE\"]\n",
    "        df[\"Total_Loss\"] = df[\"Paid_Loss\"] + df[\"OS_Loss\"]\n",
    "        return df\n",
    "\n",
    "    (claims_df, time_interval_axis_mapping) = get_time_interval_fields(\n",
    "        df, X_AXIS_DATE_FIELD, Y_AXIS_DATE_FIELD, TIME_INTERVAL\n",
    "    )\n",
    "    claims_df = get_cumulative_metrics(\n",
    "        claims_df,\n",
    "        CLAIM_NUMBER_COLUMN,\n",
    "        LOSS_COLUMNS,\n",
    "        time_interval_axis_mapping,\n",
    "        X_AXIS_DATE_FIELD,\n",
    "    )\n",
    "    claims_df = get_last_values_during_developed_period(\n",
    "        claims_df, CLAIM_NUMBER_COLUMN, LOSS_COLUMNS, time_interval_axis_mapping\n",
    "    )\n",
    "\n",
    "    claims_df = _update_insurance_totals(claims_df)\n",
    "\n",
    "    (df_quarter_claim_pivot, start_quarter, end_quarter) = get_period_claim_pivot(\n",
    "        claims_df, CLAIM_NUMBER_COLUMN, time_interval_axis_mapping, TIME_INTERVAL\n",
    "    )\n",
    "\n",
    "    claims_triangle_df = merge_with_period_claim_pivot(\n",
    "        claims_df,\n",
    "        df_quarter_claim_pivot,\n",
    "        CLAIM_NUMBER_COLUMN,\n",
    "        time_interval_axis_mapping,\n",
    "    )\n",
    "\n",
    "    (triangle_list, max_len, First_AQ_list) = get_loss_triangle_on_target_metric(\n",
    "        claims_triangle_df,\n",
    "        start_quarter,\n",
    "        end_quarter,\n",
    "        TARGET_LOSS_METRIC,\n",
    "        TIME_INTERVAL,\n",
    "        time_interval_axis_mapping,\n",
    "    )\n",
    "\n",
    "    claims_triangle = get_claims_triangle(\n",
    "        triangle_list, max_len, First_AQ_list, TIME_INTERVAL, time_interval_axis_mapping\n",
    "    )\n",
    "\n",
    "    claims_triangle_ldf = get_ldf_from_claims_triangle(claims_triangle, TIME_INTERVAL)\n",
    "\n",
    "    return (claims_triangle, claims_triangle_ldf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d622376a-08dc-4547-8eac-ec114ad08ce5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_ldf_weighted_average_and_ftu(\n",
    "    df: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    TIME_INTERVAL: str,\n",
    ") -> pd.DataFrame:\n",
    "    def _get_ldf_average(df: pd.DataFrame, TIME_INTERVAL: str) -> np.ndarray:\n",
    "        if TIME_INTERVAL == \"yearly\":\n",
    "            # df.mean() treats the \"AY\" column as a aggregation value\n",
    "            ldf_average = df.mean().values[1:]\n",
    "        else:\n",
    "            ldf_average = df.mean().values\n",
    "        return ldf_average.reshape(1, -1)\n",
    "\n",
    "    def _get_weighted_ldf_average(df: pd.DataFrame, df2: pd.DataFrame) -> np.ndarray:\n",
    "        # Get dataframe into numpy format and drop first column (TIME_INTERVAL column)\n",
    "        claim_amounts = df.iloc[:, 1:].to_numpy()\n",
    "\n",
    "        # Set last non-NaN value to NaN\n",
    "        last_non_nan_indices = np.argmax(np.isnan(claim_amounts), axis=1) - 1\n",
    "        last_non_nan_indices[last_non_nan_indices == -1] = claim_amounts.shape[1] - 1\n",
    "        claim_amounts[np.arange(claim_amounts.shape[0]), last_non_nan_indices] = np.nan\n",
    "\n",
    "        # Set NaN values to 0.0 for weighting\n",
    "        claim_amounts[np.isnan(claim_amounts)] = 0.0\n",
    "\n",
    "        # Get dataframe into numpy format and drop first column (TIME_INTERVAL column)\n",
    "        age_to_age_factors = df2.iloc[:, 1:].to_numpy()\n",
    "\n",
    "        # Get the weights\n",
    "        columnwise_summation = np.sum(claim_amounts, axis=0)\n",
    "        weights = claim_amounts / columnwise_summation[np.newaxis, :]\n",
    "\n",
    "        # Calculate the weights age-to-age factors\n",
    "        weighted_age_to_age_factors = weights * age_to_age_factors\n",
    "\n",
    "        # Calculate the summed weighted age-to-age factors\n",
    "        summed_weighted_age_to_age_factors = np.nansum(\n",
    "            weighted_age_to_age_factors, axis=0\n",
    "        ).reshape(1, -1)\n",
    "\n",
    "        return summed_weighted_age_to_age_factors\n",
    "\n",
    "    ldf_average = _get_ldf_average(df2, TIME_INTERVAL)[:, :-1]\n",
    "    weighted_ldf_average = _get_weighted_ldf_average(df, df2)[:, :-1]\n",
    "\n",
    "    def get_ftu_cell_value(weighted_ldf_average: np.ndarray) -> float:\n",
    "        ftu_cell = np.cumprod(weighted_ldf_average, axis=1)[0, -1]\n",
    "        return ftu_cell\n",
    "\n",
    "    ftu = np.array(\n",
    "        [\n",
    "            get_ftu_cell_value(weighted_ldf_average[:, i:])\n",
    "            for i in np.arange(0, len(weighted_ldf_average[0]))\n",
    "        ]\n",
    "    ).reshape(1, -1)\n",
    "\n",
    "    all_ldf_caption_values = np.hstack(\n",
    "        (\n",
    "            np.array([[\"Average\", \"Weighted Average\", \"FTU\"]]).T,\n",
    "            np.vstack((ldf_average, weighted_ldf_average, ftu)),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    ldf_averages_df = pd.DataFrame(\n",
    "        all_ldf_caption_values,\n",
    "        columns=[\"Metric\"] + df2.columns[1:-1].tolist(),\n",
    "    )\n",
    "    ldf_averages_df[df2.columns[1:-1].tolist()] = ldf_averages_df[\n",
    "        df2.columns[1:-1].tolist()\n",
    "    ].astype(float)\n",
    "    return ldf_averages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db7fb23d-3397-41d5-a1d1-af89d254f72c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_pandas_ldf_styling(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def highlight_values(val):\n",
    "        if val > 10:\n",
    "            return \"background-color: red\"\n",
    "        elif val > 2.0:\n",
    "            return \"background-color: orange\"\n",
    "        elif val > 1.00:\n",
    "            return \"background-color: yellow\"\n",
    "        elif val > 0.95:\n",
    "            return \"background-color: lime\"\n",
    "        elif val > 0.05:\n",
    "            return \"background-color: blue\"\n",
    "        else:\n",
    "            return \"background-color: none\"\n",
    "\n",
    "    styled_df = df.style.applymap(highlight_values, subset=df.columns[1:])\n",
    "\n",
    "    return styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "020ad9fe-8887-4555-986d-972d4e0f41ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run(\n",
    "    USE_STATIC_DATA: bool = True,\n",
    "    Y_AXIS_DATE_FIELDS: list[str] = [\n",
    "        \"Acc_Date\",\n",
    "        # \"Policy_Date\",\n",
    "        # \"Report_Date\",\n",
    "        # \"Treaty_Date\",\n",
    "    ],\n",
    "    TARGET_LOSS_METRICS: list[str] = [\n",
    "        \"Total_Paid\",\n",
    "        \"Total_Incurred\",\n",
    "        \"Total_ALE\",\n",
    "        \"Total_Loss\",\n",
    "    ],\n",
    "    TIME_INTERVALS: list[str] = [\"monthly\", \"quarterly\", \"yearly\"],\n",
    "    LOCAL_FILEPATH: str = \"/tmp/ldf_output_permutation_test_tempfile.xlsx\",\n",
    "    OUTPUT_DBFS_FILEPATH: str = \"/dbfs/mnt/datascience/ldf_output_permutation_test.xlsx\",\n",
    ") -> list[tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]]:\n",
    "    if USE_STATIC_DATA:\n",
    "        BOOK = \"King\"\n",
    "        (\n",
    "            FULL_CLAIMS_DF,\n",
    "            X_AXIS_DATE_FIELD,\n",
    "            CLAIM_NUMBER_COLUMN,\n",
    "            LOSS_COLUMNS,\n",
    "        ) = get_static_data(BOOK)\n",
    "\n",
    "    triangle_outputs = []\n",
    "    with pd.ExcelWriter(LOCAL_FILEPATH, engine=\"openpyxl\") as writer:\n",
    "        for idx, combination in enumerate(itertools.product(\n",
    "            Y_AXIS_DATE_FIELDS, TARGET_LOSS_METRICS, TIME_INTERVALS\n",
    "        )):\n",
    "            (Y_AXIS_DATE_FIELD, TARGET_LOSS_METRIC, TIME_INTERVAL) = combination\n",
    "\n",
    "            (claims_triangle, claims_triangle_ldf) = get_loss_triangles_from_parameters(\n",
    "                FULL_CLAIMS_DF,\n",
    "                X_AXIS_DATE_FIELD,\n",
    "                Y_AXIS_DATE_FIELD,\n",
    "                CLAIM_NUMBER_COLUMN,\n",
    "                LOSS_COLUMNS,\n",
    "                TARGET_LOSS_METRIC,\n",
    "                TIME_INTERVAL,\n",
    "            )\n",
    "\n",
    "            ldf_averages_df = get_ldf_weighted_average_and_ftu(\n",
    "                claims_triangle, claims_triangle_ldf, TIME_INTERVAL\n",
    "            )\n",
    "\n",
    "            triangle_outputs.append(\n",
    "                (claims_triangle, claims_triangle_ldf, ldf_averages_df)\n",
    "            )\n",
    "\n",
    "            combined_df = pd.concat(\n",
    "                [claims_triangle, claims_triangle_ldf, ldf_averages_df],\n",
    "                axis=1,\n",
    "                keys=[\"claims_triangle\", \"claims_triangle_ldf\", \"ldf_averages_df\"],\n",
    "            )\n",
    "\n",
    "            sheet_name = f\"comb_{idx}\"\n",
    "            combined_df.to_excel(writer, sheet_name=sheet_name)\n",
    "\n",
    "    copyfile(LOCAL_FILEPATH, OUTPUT_DBFS_FILEPATH)\n",
    "\n",
    "    return triangle_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "543d11a2-4cc7-489e-9ef1-56d51068b9c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "triangle_outputs = run()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Loss Development Triangle Automation",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
